{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651fef7a",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# The Base Classification Model\n",
    ":label:`sec_classification`\n",
    "\n",
    "You may have noticed that the implementations from scratch and the concise implementation using framework functionality were quite similar in the case of regression. The same is true for classification. Since many models in this book deal with classification, it is worth adding functionalities to support this setting specifically. This section provides a base class for classification models to simplify future code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1dd1359",
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bc0d4e",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## The `Classifier` Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641cbe3d",
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "We define the `Classifier` class below. \n",
    "\n",
    "- In the `validation_step` we report both the loss value and the classification accuracy on a validation batch.\n",
    "\n",
    "We draw an update for every `num_val_batches` batches. This has the benefit of generating the averaged loss and accuracy on the whole validation data. These average numbers are not exactly correct if the final batch contains fewer examples, but we ignore this minor difference to keep the code simple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c1376-570e-496b-b25e-7504ee9177f8",
   "metadata": {},
   "source": [
    "Let's revisit the `Module`:\n",
    "\n",
    "<img src=\"../images/module.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89e1b7-261c-4a92-9130-6b7bb818683b",
   "metadata": {},
   "source": [
    "So, basically it says **what happens when you do the training step** and **val step**. You will configure the stuff here. Like forward, loss etc.\n",
    "\n",
    "#### Why are we doing this for classifier?\n",
    "- See, in the **original** implementation we only report the \"loss\" but not accuracy.\n",
    "- We need to have both, but just for the validation. And not for training, thus we do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb74037",
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Classifier(d2l.Module):  #@save\n",
    "    \"\"\"\n",
    "    Overriding the validation step method which will \n",
    "    plot loss and the accuracy.\n",
    "    \"\"\"\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5244291",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "By default we use a stochastic gradient descent optimizer, operating on minibatches, just as we did in the context of linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d1c1bb6",
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(Classifier)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return torch.optim.SGD(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28232a31",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "## Accuracy\n",
    "- **Accuracy calculation process**:\n",
    "  1. If `y_hat` is a matrix, it contains prediction scores for each class.\n",
    "  2. **Use `argmax`** to get the predicted class (index of the largest value) for each row.\n",
    "  3. **Compare** predicted classes with the true labels (`y`) elementwise.\n",
    "  4. Convert `y_hat`'s data type to match `y` for consistent comparison.\n",
    "  5. **Result**: A tensor of 0s (false) and 1s (true).\n",
    "  6. **Summing** these values gives the number of correct predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b132abd8",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(Classifier)  #@save\n",
    "def accuracy(self, Y_hat, Y, averaged=True):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n",
    "    preds = Y_hat.argmax(axis=1).type(Y.dtype)\n",
    "    compare = (preds == Y.reshape(-1)).type(torch.float32)\n",
    "    return compare.mean() if averaged else compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47696b41",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "## Summary\n",
    "\n",
    "Classification is a sufficiently common problem that it warrants its own convenience functions. Of central importance in classification is the *accuracy* of the classifier. Note that while we often care primarily about accuracy, we train classifiers to optimize a variety of other objectives for statistical and computational reasons. However, regardless of which loss function was minimized during training, it is useful to have a convenience method for assessing the accuracy of our classifier empirically. \n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Denote by $L_\\textrm{v}$ the validation loss, and let $L_\\textrm{v}^\\textrm{q}$ be its quick and dirty estimate computed by the loss function averaging in this section. Lastly, denote by $l_\\textrm{v}^\\textrm{b}$ the loss on the last minibatch. Express $L_\\textrm{v}$ in terms of $L_\\textrm{v}^\\textrm{q}$, $l_\\textrm{v}^\\textrm{b}$, and the sample and minibatch sizes.\n",
    "1. Show that the quick and dirty estimate $L_\\textrm{v}^\\textrm{q}$ is unbiased. That is, show that $E[L_\\textrm{v}] = E[L_\\textrm{v}^\\textrm{q}]$. Why would you still want to use $L_\\textrm{v}$ instead?\n",
    "1. Given a multiclass classification loss, denoting by $l(y,y')$ the penalty of estimating $y'$ when we see $y$ and given a probabilty $p(y \\mid x)$, formulate the rule for an optimal selection of $y'$. Hint: express the expected loss, using $l$ and $p(y \\mid x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f847c",
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/6809)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
