{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fef961",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# 1️⃣ Layers and Modules\n",
    "\n",
    "### 👇🏻 \n",
    "There are the times **when we need to have an access to the repetable structures** to create complicated networks easiliy and with more managable way.\n",
    "\n",
    "1. The thing we often need an access to in a *modular* way is not the layer and also not a whole model... it is somewhere in between. Possibly a **group of layers**.\n",
    "2. Also **we may need to tweak the `forward` operation** in the way we like -- instead just the simple matmul. We may want to do some programming in between or change some input or do some operations before producing the output.\n",
    "\n",
    "These all can be achieved through **grouping the layers** and giving them a **special treatment**. This is done through `nn.Module` class.\n",
    "\n",
    "---\n",
    "\n",
    "Here, we will first see `what` and then we will have a look at `how`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b911ed7",
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80abfaa",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "The following code generates a network\n",
    "with one fully connected hidden layer\n",
    "with 256 units and ReLU activation,\n",
    "followed by a fully connected output layer\n",
    "with ten units (no activation function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df830d6",
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "\n",
    "# the standard forward\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93754877",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## A Custom Module\n",
    "\n",
    "Perhaps the easiest way to develop intuition\n",
    "about how a module works\n",
    "is to implement one ourselves.\n",
    "Before we do that,\n",
    "we briefly summarize the basic functionality\n",
    "that each module must provide:\n",
    "\n",
    "\n",
    "1. **Ingest input data as arguments** to its forward propagation method.\n",
    "1. Generate **an output** by having the **forward propagation** method **return a value**. Note that the output may have a different shape from the input. For example, the first fully connected layer in our model above ingests an input of arbitrary dimension but returns an output of dimension 256.\n",
    "1. Calculate the gradient of its output with respect to its input, which can be accessed via its backpropagation method. Typically this happens automatically.\n",
    "1. Store and provide access to those parameters necessary\n",
    "   for executing the forward propagation computation.\n",
    "1. Initialize model parameters as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69437a44-3213-4fbb-8bae-41c5ee29ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform\n",
    "        # the necessary initialization\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "\n",
    "    # **define** the forward propogation of the model, that is, how to return the\n",
    "    # required model output based on the input X\n",
    "    def forward(self, X):\n",
    "        h = self.hidden(X)\n",
    "        h = F.relu(h)\n",
    "        o = self.out(h)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8301dc",
   "metadata": {
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cef5ea",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "**A key virtue of the module abstraction is its versatility. We can subclass a module to create layers (such as the fully connected layer class), entire models (such as the `MLP` class above), or various components of intermediate complexity.**\n",
    "\n",
    "## **The Sequential Module**\n",
    "\n",
    "We can now take a closer look\n",
    "at how the `Sequential` class works.\n",
    "Recall that `Sequential` was designed\n",
    "to daisy-chain other modules together.\n",
    "To build our own simplified `MySequential`,\n",
    "we just need to define two key methods:\n",
    "\n",
    "1. A method for appending modules one by one to a list.\n",
    "1. A forward propagation method for passing an input through the chain of modules, in the same order as they were appended.\n",
    "\n",
    "The following `MySequential` class delivers the same\n",
    "functionality of the default `Sequential` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b7f913",
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a2da4",
   "metadata": {
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "In the `__init__` method, we add every module\n",
    "by calling the `add_modules` method. These modules can be accessed by the `children` method at a later date.\n",
    "In this way the system knows the added modules,\n",
    "and it will properly initialize each module's parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008229d2",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "When our `MySequential`'s forward propagation method is invoked,\n",
    "each added module is executed\n",
    "in the order in which they were added.\n",
    "We can now reimplement an MLP\n",
    "using our `MySequential` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8d5d6a",
   "metadata": {
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fc5fe",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "## **Executing Code in the Forward Propagation Method**\n",
    "\n",
    "> This is the way to **manipulate what happens in the `forward` method**\n",
    "\n",
    "The `Sequential` class makes model construction easy,\n",
    "allowing us to assemble new architectures\n",
    "without having to define our own class.\n",
    "However, not all architectures are simple daisy chains.\n",
    "When greater flexibility is required,\n",
    "we will want to define our own blocks.\n",
    "For example, we might want to execute\n",
    "Python's control flow within the forward propagation method.\n",
    "Moreover, we might want to perform\n",
    "arbitrary mathematical operations,\n",
    "not simply relying on predefined neural network layers.\n",
    "\n",
    "You may have noticed that until now,\n",
    "all of the operations in our networks\n",
    "have acted upon our network's activations\n",
    "and its parameters.\n",
    "Sometimes, however, we might want to\n",
    "incorporate terms\n",
    "that are neither the result of previous layers\n",
    "nor updatable parameters.\n",
    "We call these *constant parameters*.\n",
    "Say for example that we want a layer\n",
    "that calculates the function\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$,\n",
    "where $\\mathbf{x}$ is the input, $\\mathbf{w}$ is our parameter,\n",
    "and $c$ is some specified constant\n",
    "that is not updated during optimization.\n",
    "So we implement a `FixedHiddenMLP` class as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f8721f0",
   "metadata": {
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e65b0b",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "In this model,\n",
    "we implement a hidden layer whose weights\n",
    "(`self.rand_weight`) are initialized randomly\n",
    "at instantiation and are thereafter constant.\n",
    "This weight is not a model parameter\n",
    "and thus it is never updated by backpropagation.\n",
    "The network then passes the output of this \"fixed\" layer\n",
    "through a fully connected layer.\n",
    "\n",
    "**Note that before returning the output,\n",
    "our model did something unusual.**\n",
    "\n",
    "We ran a while-loop, testing\n",
    "on the condition its $\\ell_1$ norm is larger than $1$,\n",
    "and dividing our output vector by $2$\n",
    "until it satisfied the condition.\n",
    "Finally, we returned the sum of the entries in `X`.\n",
    "\n",
    "> ⚠️ To our knowledge, ***no standard neural network\n",
    "performs this operation.*** Note that this particular operation may not be useful\n",
    "in any real-world task.\n",
    "Our point is only to show you how to integrate\n",
    "arbitrary code into the flow of your\n",
    "neural network computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ede5347f",
   "metadata": {
    "origin_pos": 40,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0446, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343c1a3",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "We can **mix and match various\n",
    "ways of assembling modules together.**\n",
    "In the following example, we nest modules\n",
    "in some creative ways.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c3d190",
   "metadata": {
    "origin_pos": 44,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2316, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571d4ae-a671-4163-bff5-028c835b476e",
   "metadata": {},
   "source": [
    "# 2️⃣ Accessing the Parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01398665-898e-416b-849d-aaabccb5b2d5",
   "metadata": {},
   "source": [
    "Once we have chosen an architecture\n",
    "and set our hyperparameters,\n",
    "we proceed to the training loop,\n",
    "where **our goal is to find parameter values\n",
    "that minimize our loss function.**\n",
    "\n",
    "\n",
    "After training, we will need these parameters\n",
    "in order to make future predictions.\n",
    "Additionally, we will sometimes wish\n",
    "to extract the parameters\n",
    "perhaps to reuse them in some other context,\n",
    "to save our model to disk so that\n",
    "it may be executed in other software,\n",
    "or for examination in the hope of\n",
    "gaining scientific understanding.\n",
    "\n",
    "Most of the time, we will be able\n",
    "to ignore the nitty-gritty details\n",
    "of how parameters are declared\n",
    "and manipulated, relying on deep learning frameworks\n",
    "to do the heavy lifting.\n",
    "However, when we move away from\n",
    "stacked architectures with standard layers,\n",
    "we will sometimes need to get into the weeds\n",
    "of declaring and manipulating parameters.\n",
    "In this section, we cover the following:\n",
    "\n",
    "* Accessing parameters for debugging, diagnostics, and visualizations.\n",
    "* Sharing parameters across different model components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e8adc9d-794a-4c58-ad4b-433b6c039e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37697ff7-eda2-475f-b3ab-5d6584090424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade964c-2b26-4ff2-86c7-ea8915faa669",
   "metadata": {},
   "source": [
    "It is as simple as it gets. The data has 2 rows and 4 features. The net has single hidden layer which has 8 nodes (double the features) and single output.\n",
    "\n",
    "Thus the prediction has `2 x 1` shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8949d-f8e2-4e06-adcb-8801e94d7509",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "## **Parameter Access**\n",
    "\n",
    "Let's start with how to access parameters\n",
    "from the models that you already know.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32af8c3-417e-4033-95d8-438ba7869863",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "When a model is defined via the `Sequential` class,\n",
    "we can first access any layer by indexing\n",
    "into the model **as though it were a list**.\n",
    "Each layer's parameters are conveniently\n",
    "located in its attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f3d376-b847-4cf3-91fe-b3ec63dbb97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.2271,  0.3601,  0.0261,  0.4770],\n",
       "                      [ 0.0050, -0.1673,  0.4541,  0.2741],\n",
       "                      [ 0.4006,  0.1460,  0.2844,  0.3897],\n",
       "                      [-0.4409, -0.3244, -0.1448,  0.3665],\n",
       "                      [-0.4509,  0.2230,  0.0334,  0.0913],\n",
       "                      [ 0.1658,  0.2303, -0.4725, -0.4774],\n",
       "                      [ 0.2010, -0.0702,  0.4590, -0.1829],\n",
       "                      [ 0.2476,  0.4240,  0.0115, -0.0101]])),\n",
       "             ('bias',\n",
       "              tensor([-0.0299, -0.4780,  0.2742,  0.3652, -0.3704,  0.0021, -0.2558, -0.3816]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden layer weights \n",
    "net[0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2561f42-caa4-49b3-9513-a828d7b16f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.2736, -0.0023,  0.0856, -0.0967,  0.0933, -0.2383, -0.2015, -0.2272]])),\n",
       "             ('bias', tensor([-0.2758]))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last layer weights\n",
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4f65b16-5efd-44da-9e13-7f8e97c31925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d7faeb8-a4aa-4a67-a474-1bffbc8d4bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2271,  0.3601,  0.0261,  0.4770],\n",
       "        [ 0.0050, -0.1673,  0.4541,  0.2741],\n",
       "        [ 0.4006,  0.1460,  0.2844,  0.3897],\n",
       "        [-0.4409, -0.3244, -0.1448,  0.3665],\n",
       "        [-0.4509,  0.2230,  0.0334,  0.0913],\n",
       "        [ 0.1658,  0.2303, -0.4725, -0.4774],\n",
       "        [ 0.2010, -0.0702,  0.4590, -0.1829],\n",
       "        [ 0.2476,  0.4240,  0.0115, -0.0101]], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TYPE: Parameter\n",
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eeeb80c5-1f46-4a74-8906-0327be8b60ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2271,  0.3601,  0.0261,  0.4770],\n",
       "        [ 0.0050, -0.1673,  0.4541,  0.2741],\n",
       "        [ 0.4006,  0.1460,  0.2844,  0.3897],\n",
       "        [-0.4409, -0.3244, -0.1448,  0.3665],\n",
       "        [-0.4509,  0.2230,  0.0334,  0.0913],\n",
       "        [ 0.1658,  0.2303, -0.4725, -0.4774],\n",
       "        [ 0.2010, -0.0702,  0.4590, -0.1829],\n",
       "        [ 0.2476,  0.4240,  0.0115, -0.0101]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TYPE: Tensor\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a43f17c8-cd12-4058-98c4-700d961fb687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.grad == None #?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b735e0-062c-44c8-86c0-3511800f56a5",
   "metadata": {},
   "source": [
    "> 🔥 The `.state_dict()` is the OP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858efb3-8e14-4401-b390-2e7694026a5d",
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "👉🏻 **Parameters are complex objects,\n",
    "containing values, gradients,\n",
    "and additional information.**\n",
    "That is why we need to request the value explicitly.\n",
    "\n",
    "In addition to the value, each parameter also allows us to access the gradient. Because we have not invoked backpropagation for this network yet, it is in its initial state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf0c3b-34ae-4f1a-94d3-6bd90f48e8ab",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "### ⭐ **All Parameters at Once**\n",
    "\n",
    "When we need to perform operations on all parameters,\n",
    "accessing them one-by-one can grow tedious.\n",
    "The situation can grow especially unwieldy\n",
    "when we work with more complex, e.g., nested, modules,\n",
    "since we would need to recurse\n",
    "through the entire tree to extract\n",
    "each sub-module's parameters. Below we demonstrate accessing the parameters of all layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34f64140-a88c-4ff4-96e5-e91aab6be27d",
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight', torch.Size([8, 4])),\n",
       " ('0.bias', torch.Size([8])),\n",
       " ('2.weight', torch.Size([1, 8])),\n",
       " ('2.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, param.shape) for name, param in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f7281-d0ae-4ba8-92df-bd1d9fb24a20",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "## 🖇️ **Tied (SHARED) Parameters**\n",
    "\n",
    "Often, we want to ***share parameters*** across multiple layers.\n",
    "Let's see how to do this elegantly.\n",
    "\n",
    "In the following we allocate a fully connected layer\n",
    "and then use its parameters specifically\n",
    "to set those of another layer.\n",
    "Here we need to run the forward propagation\n",
    "`net(X)` before accessing the parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c5dac-2a9d-409a-a5d7-f40ff145754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4b982-3828-4dc0-acbf-62cb1e6c5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa58676-1b5b-4200-a661-2220786c0ce9",
   "metadata": {},
   "source": [
    "We have simply picked up from the following:\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        nn.LazyLinear(8), # 0 \n",
    "        nn.ReLU(),        # 1\n",
    "        shared,           # 2\n",
    "        nn.ReLU(),        # 3\n",
    "        shared,           # 4\n",
    "        nn.ReLU(),        # 5\n",
    "        nn.LazyLinear(1)  # 6\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b7724-2170-4478-9baa-876e3b2684fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as doing it with [4].\n",
    "net[2].weight.data[0, 0] = 100\n",
    "\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5d90a-1ba6-4d45-b5c7-a2e2ad588134",
   "metadata": {},
   "source": [
    "### The essence of the shared weights.\n",
    "\n",
    "See, these are the *shared weights* and not *shared nodes*. Here, we **reuse the weights**. \n",
    "\n",
    "***Thus, it looks like the following 👇🏻***\n",
    "\n",
    "<img src=\"../images/shared-weights.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ee2bd-df1b-4544-b0e0-e534f93f6986",
   "metadata": {},
   "source": [
    "# Next up,\n",
    "We will see the initializers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
