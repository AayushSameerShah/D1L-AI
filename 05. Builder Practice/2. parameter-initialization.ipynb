{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0e65bf",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Parameter Initialization\n",
    "\n",
    "Now that we know how to access the parameters,\n",
    "let's look at how to initialize them properly.\n",
    "\n",
    "\n",
    "However, we often want to initialize our weights\n",
    "according to various other protocols. The framework provides most commonly\n",
    "used protocols, and also allows to create a custom initializer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c14f80",
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e66e5",
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "By default, PyTorch initializes weight and bias matrices\n",
    "uniformly by drawing from a range that is computed according to the input and output dimension.\n",
    "PyTorch's `nn.init` module provides a variety\n",
    "of preset initialization methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a5e8cb",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9c8b0",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## **Built-in Initialization**\n",
    "\n",
    "Let's begin by calling on built-in initializers.\n",
    "The code below initializes all weight parameters\n",
    "as Gaussian random variables\n",
    "with standard deviation 0.01, while bias parameters are cleared to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6059e0fb",
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0068,  0.0050, -0.0197, -0.0054]), tensor(0.))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.normal_(module.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db641d-f405-44ef-92f7-ca6324980721",
   "metadata": {},
   "source": [
    "### What was that!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f744a75-cdfa-400a-aa39-39a769eeb3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look, this `net` is sequential and it is a module\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6625aee-98ca-43d4-bbb6-415fd31e31d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.container.Sequential"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See? This the Sequential is the type \"module\"\n",
    "type(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef85e44-7ff9-445c-88a9-22e16cbd0fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.linear.Linear"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See? This is the type linear\n",
    "type(net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a589ad6-9f3a-4dd7-a62e-31e943deda28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Fills the input Tensor with values drawn from the normal\n",
       "distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`.\n",
       "\n",
       "Args:\n",
       "    tensor: an n-dimensional `torch.Tensor`\n",
       "    mean: the mean of the normal distribution\n",
       "    std: the standard deviation of the normal distribution\n",
       "\n",
       "Examples:\n",
       "    >>> w = torch.empty(3, 5)\n",
       "    >>> nn.init.normal_(w)\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\aayus\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\init.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.init.normal_?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b2318-9ccf-4f16-be9f-97a656f58fb8",
   "metadata": {},
   "source": [
    "> ### ðŸ“– Did you read it?\n",
    "> It ***Fills the input Tensor with values drawn from the normal***.\n",
    ">\n",
    "> It FILLs the EXISTING tensor with the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17dc58-87c3-4b16-8c12-daf2a304bad4",
   "metadata": {},
   "source": [
    "## Why to check `nn.Linear`?\n",
    "\n",
    "In the `init_normal` function:\n",
    "```python\n",
    "def init_normal(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.normal_(module.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(module.bias)\n",
    "```\n",
    "\n",
    "They check for `nn.Linear` because:\n",
    "1. **Selective Initialization**:  (ðŸ‘ˆðŸ»ðŸ‘ˆðŸ»ðŸ‘ˆðŸ»)\n",
    "   - The model (`net`) contains multiple layers, including `nn.ReLU()`, which does not have weights or biases.\n",
    "   - Initialization only makes sense for layers like `nn.Linear` that **have trainable parameters** (weights and biases).\n",
    "2. **Avoid Errors**:\n",
    "   - If they tried initializing `module.weight` or `module.bias` on a layer without these attributes (like `nn.ReLU`), it would raise an error.\n",
    "\n",
    "---\n",
    "\n",
    "### What Are Other Layers in PyTorch Besides `nn.Linear`?\n",
    "\n",
    "There are many types of layers in PyTorch, each serving different purposes. Here's an overview:\n",
    "\n",
    "#### 1. **Linear Layers**:\n",
    "   - `nn.Linear`: Fully connected (dense) layer for regular feedforward neural networks.\n",
    "\n",
    "#### 2. **Activation Layers**:\n",
    "   - `nn.ReLU`: Rectified Linear Unit activation.\n",
    "   - `nn.Sigmoid`: Sigmoid activation.\n",
    "   - `nn.Tanh`: Hyperbolic tangent activation.\n",
    "   - `nn.Softmax`: Normalizes output to probabilities.\n",
    "\n",
    "#### 3. **Convolutional Layers**:\n",
    "   - `nn.Conv1d`: 1D convolution layer (e.g., for time-series data).\n",
    "   - `nn.Conv2d`: 2D convolution layer (e.g., for images).\n",
    "   - `nn.Conv3d`: 3D convolution layer (e.g., for volumetric data).\n",
    "\n",
    "#### 4. **Pooling Layers**:\n",
    "   - `nn.MaxPool2d`: 2D max-pooling layer.\n",
    "   - `nn.AvgPool2d`: 2D average-pooling layer.\n",
    "   - `nn.AdaptiveAvgPool2d`: Pooling layer that outputs a specified size, regardless of input size.\n",
    "\n",
    "#### 5. **Recurrent Layers**:\n",
    "   - `nn.RNN`: Basic recurrent neural network layer.\n",
    "   - `nn.LSTM`: Long Short-Term Memory layer (handles sequences with memory).\n",
    "   - `nn.GRU`: Gated Recurrent Unit (a simpler alternative to LSTM).\n",
    "\n",
    "#### 6. **Normalization Layers**:\n",
    "   - `nn.BatchNorm1d`: Batch normalization for 1D inputs.\n",
    "   - `nn.LayerNorm`: Layer normalization.\n",
    "   - `nn.InstanceNorm2d`: Instance normalization for images.\n",
    "\n",
    "#### 7. **Dropout Layers**:\n",
    "   - `nn.Dropout`: Randomly drops out elements in the input.\n",
    "   - `nn.AlphaDropout`: Special dropout for self-normalizing neural networks (like SELU activations).\n",
    "\n",
    "#### 8. **Embedding Layers**:\n",
    "   - `nn.Embedding`: Maps discrete indices (e.g., word IDs) to dense vectors.\n",
    "\n",
    "#### 9. **Transformers and Attention Layers**:\n",
    "   - `nn.Transformer`: Implements the Transformer architecture.\n",
    "   - `nn.MultiheadAttention`: Multi-head attention mechanism.\n",
    "\n",
    "#### 10. **Custom Layers**:\n",
    "   - You can also define your own layers by subclassing `nn.Module`.\n",
    "\n",
    "---\n",
    "\n",
    "### How Does This Relate to `init_normal`?\n",
    "\n",
    "The `init_normal` function specifically initializes **only the weights and biases of layers that have them**, such as `nn.Linear`. It skips other layers, like `nn.ReLU`, which donâ€™t require initialization.\n",
    "\n",
    "If you wanted to initialize parameters for other layers (e.g., `nn.Conv2d`), you would need to extend the `if type(module) == ...` logic accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa39cb-e38b-4b94-9645-eb75c511cb9d",
   "metadata": {},
   "source": [
    "> ## ðŸ’­\n",
    "> That means the `.apply()` passes each layer in seperately!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e91fa",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "We can also initialize all the parameters\n",
    "to a given constant value (say, 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2007d64",
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f5980-26a3-41d0-bd24-1cf57cbd7d64",
   "metadata": {},
   "source": [
    "# Till Now...\n",
    "\n",
    "We tried to see how to initialize the layer **formally** with the random values -- still with the normal distribution *but* using `nn.init` -- which is the standard way of doing that.\n",
    "\n",
    "**Now, we will see** how to use any of the formal **initialization methods** to do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ca37e",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "## **We can also apply <u>different initializers</u> for certain blocks.**\n",
    "\n",
    "For example, below we initialize the first layer\n",
    "with the Xavier initializer\n",
    "and initialize the second layer\n",
    "to a constant value of 42.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4734e6eb",
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4750, -0.4546, -0.3507,  0.2501])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "def init_42(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.constant_(module.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e647d0-8382-401f-9e56-7484ecb43a70",
   "metadata": {},
   "source": [
    "### How `apply` works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea608e31-e9d1-41ac-887a-2fc16cac6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = net[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5281c61-ce0c-4cfe-87e9-4f7f5b3172e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_apply(module):\n",
    "    print(\"This is:\", type(module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2300fcb-1dcc-42d9-9357-fc6419a255d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is: <class 'torch.nn.modules.linear.Linear'>\n",
      "This is: <class 'torch.nn.modules.activation.ReLU'>\n",
      "This is: <class 'torch.nn.modules.linear.Linear'>\n",
      "This is: <class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "net.apply(see_apply);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164af268",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "### **Custom Initialization**\n",
    "\n",
    "Sometimes, the initialization methods we need\n",
    "are not provided by the deep learning framework.\n",
    "In the example below, we define an initializer\n",
    "for any weight parameter $w$ using the following strange distribution:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\textrm{ with probability } \\frac{1}{4} \\\\\n",
    "            0    & \\textrm{ with probability } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\textrm{ with probability } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac721402",
   "metadata": {
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Again, we implement a `my_init` function to apply to `net`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "334b9bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:43:11.159032Z",
     "iopub.status.busy": "2023-08-18T19:43:11.158501Z",
     "iopub.status.idle": "2023-08-18T19:43:11.166911Z",
     "shell.execute_reply": "2023-08-18T19:43:11.166067Z"
    },
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -7.6364, -0.0000, -6.1206],\n",
       "        [ 9.3516, -0.0000,  5.1208, -8.4003]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in module.named_parameters()][0])\n",
    "        nn.init.uniform_(module.weight, -10, 10)\n",
    "        module.weight.data *= module.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c13b0",
   "metadata": {
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Note that we always have the option\n",
    "of setting parameters directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38feecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:43:11.170212Z",
     "iopub.status.busy": "2023-08-18T19:43:11.169683Z",
     "iopub.status.idle": "2023-08-18T19:43:11.176291Z",
     "shell.execute_reply": "2023-08-18T19:43:11.175385Z"
    },
    "origin_pos": 41,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000, -6.6364,  1.0000, -5.1206])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a694ff-2f5f-486b-b053-9b853706d4f3",
   "metadata": {},
   "source": [
    "# Lazy initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0472a2a6-1887-44da-878a-6beb66fab5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adc7c7df-a61e-4cdc-afa2-1a28176daf22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "464ee3d5-bd89-4507-9338-d5dd695179f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a643dd-5761-416d-9b66-a352e91d6361",
   "metadata": {},
   "source": [
    "Initially they are not initialized... (haha, the irony.) Once we pass the data... it does the initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ba186c8-e617-49a7-947c-8b81cce640c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2868, -0.2500,  0.1342, -0.2411,  0.0015,  0.0318, -0.1633, -0.0479,\n",
       "          0.0599,  0.2075],\n",
       "        [ 0.2482, -0.3402,  0.0272, -0.1896,  0.2319,  0.0018, -0.2308,  0.0148,\n",
       "         -0.0428,  0.2038]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7537b01b-b8c7-4696-97ca-70c294db5413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 4])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e0b7bfd-54af-40e2-8a0f-4950ce22cad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c7c96-2bc1-4400-ab26-dcf8cd1e8ec8",
   "metadata": {},
   "source": [
    "## What if we pass different shaped data!? *(will give error)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10daff2f-51bd-4bea-b8e4-7dd5c18eaba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x10 and 4x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x10 and 4x256)"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(2, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c0d0c-d4d2-4a51-b04c-300f5e9e572c",
   "metadata": {},
   "source": [
    "## ðŸ¤¯ Cool, right!?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748db99-19c3-4b7d-a125-22d6a177c01b",
   "metadata": {},
   "source": [
    "# If we **didn't use** the lazy one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "967439bd-1b8b-48cd-aecb-39839568319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(10, 256), nn.ReLU(), nn.Linear(256, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "273f35a2-e7a1-4f1e-9af6-58249e7eac6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0469,  0.1504, -0.2587,  ..., -0.2354, -0.0803, -0.2436],\n",
       "        [-0.0101,  0.0106,  0.2719,  ...,  0.0333,  0.1952, -0.3083],\n",
       "        [-0.2032, -0.2326,  0.0322,  ..., -0.2869,  0.1113, -0.1812],\n",
       "        ...,\n",
       "        [-0.0319, -0.2055,  0.2413,  ..., -0.1496, -0.0833,  0.1338],\n",
       "        [-0.0740, -0.2955,  0.1789,  ..., -0.0697, -0.2620,  0.1696],\n",
       "        [ 0.1776,  0.1022, -0.2120,  ..., -0.1148, -0.1557,  0.0234]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c22278c1-afd8-489c-9947-f155240959db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0378, -0.0440, -0.0553,  ...,  0.0443,  0.0069,  0.0415],\n",
       "        [ 0.0251, -0.0255,  0.0154,  ...,  0.0005,  0.0061,  0.0593],\n",
       "        [ 0.0385, -0.0234,  0.0311,  ..., -0.0506,  0.0076, -0.0358],\n",
       "        ...,\n",
       "        [ 0.0095, -0.0243, -0.0069,  ..., -0.0271, -0.0247,  0.0382],\n",
       "        [ 0.0410,  0.0261,  0.0582,  ..., -0.0459, -0.0078,  0.0260],\n",
       "        [ 0.0171, -0.0460, -0.0417,  ..., -0.0047,  0.0191,  0.0323]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad49f1-03e1-4552-a0f8-8efbadcdaffe",
   "metadata": {},
   "source": [
    "## ðŸ¤¯ It works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309c873-e2db-473f-88d4-2c40fb8ee6c0",
   "metadata": {},
   "source": [
    "# What if we use the Xavier, before passing the X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "365efc62-94b4-4b35-9d1b-34d164afe8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LazyLinear(in_features=0, out_features=10, bias=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_xavier(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "def init_42(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.constant_(module.weight, 42)\n",
    "\n",
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6443158b-8379-4530-8154-525af097943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UninitializedParameter>\n",
      "<UninitializedParameter>\n"
     ]
    }
   ],
   "source": [
    "print(net[0].weight)\n",
    "print(net[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6a0c6-bd63-42d1-aa05-a35a4b87fcee",
   "metadata": {},
   "source": [
    "Ahum. **We first need to pass the data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e812c7c4",
   "metadata": {
    "origin_pos": 43
   },
   "source": [
    "# Summary\n",
    "\n",
    "We can initialize parameters using built-in and custom initializers.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Look up the online documentation for more built-in initializers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
