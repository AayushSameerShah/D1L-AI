{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7e623c-75b7-46c1-a10b-bffca44d2602",
   "metadata": {},
   "source": [
    "# So, the thing is that --- what if we ar given constant weights!?\n",
    "\n",
    "Let's create a simple dataset, and just do a single forward pass -- then we will see how the back prop changes the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a010eab-a92e-41c7-80d2-49bdc6e47912",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6867883-1c0a-4792-a1a4-5f0549f0cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Parameters\n",
    "n_samples = 1000  # Number of samples\n",
    "n_features = 5     # Number of features\n",
    "noise = 0.1        # Standard deviation of the gaussian noise\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y, coefficients = make_regression(n_samples=n_samples, \n",
    "                                     n_features=n_features, \n",
    "                                     noise=noise, \n",
    "                                     coef=True, \n",
    "                                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d2338c6-3c91-4c38-a12a-d6bf6041618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).type(torch.float32)\n",
    "y = torch.from_numpy(y).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4c5ed49c-ecf9-4046-87a3-f0990e5e7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: *CONSTANT WEIGHTS*\n",
    "weights = torch.full((n_features, 1), 0.42, requires_grad=True) # fill with constant\n",
    "bias = torch.tensor(.123, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a66db1a-975d-49fd-9df1-12b3b960df53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200]], requires_grad=True),\n",
       " tensor(0.1230, requires_grad=True))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b016b572-3d83-4348-976f-9748d9316979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 2: forward pass\n",
    "prediction = torch.matmul(X, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38e1c673-7915-4177-a641-34cef82de824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1458],\n",
       "        [-0.1861],\n",
       "        [-0.2003],\n",
       "        [-1.0051],\n",
       "        [-1.6343]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial prediction\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "073f4767-a8a6-47ac-9062-3f3427ac3982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "581133fd-e6a1-45d3-8682-70dcf48831e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1f47886f-5868-4e77-9e3a-ca65edd9d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c3d1a329-62ae-4120-9e2f-b6d6f98d7bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " tensor([[0.3611],\n",
       "         [0.4411],\n",
       "         [0.4365],\n",
       "         [0.3894],\n",
       "         [0.4393]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "type(weights.grad), weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a933db8-d9e6-425e-9521-f189e2e9f102",
   "metadata": {},
   "source": [
    "Apart from having the **exactly the same weights** now we have different grads for each weights. That's precisely because we have different features.\n",
    "\n",
    "## Let's make the features the same as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "30d9a401-1d01-4069-aea1-f6072271fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones(1000, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4d1665f2-8f91-442f-b626-c0fd5a14a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: *CONSTANT WEIGHTS*\n",
    "weights = torch.full((n_features, 1), 0.42, requires_grad=True) # fill with constant\n",
    "bias = torch.tensor(.123, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2588d33c-05a5-4b5f-83d4-fd6f156d811d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200]], requires_grad=True),\n",
       " tensor(0.1230, requires_grad=True))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1d62c06-7f8c-460b-a71a-96a8958012c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 2: forward pass\n",
    "prediction = torch.matmul(X, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f0cd4870-b796-4af0-a602-5d74ae0199be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2230],\n",
       "        [2.2230],\n",
       "        [2.2230],\n",
       "        [2.2230],\n",
       "        [2.2230]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial prediction\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbd9f5-3790-4516-a21a-101a4f42e332",
   "metadata": {},
   "source": [
    "> See? The predictions are the same... that's expected as all values are `1`.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f692abcc-39ef-4137-89af-0b6e1afee0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5ee0f0dc-e444-47d1-a251-80fb7460cc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f08d4022-31d1-4ae2-b8ea-3fcb3f914d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59d90340-c5d6-4d31-aa8d-c6c7378a462c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " tensor([[1.3253],\n",
       "         [1.3253],\n",
       "         [1.3253],\n",
       "         [1.3253],\n",
       "         [1.3253]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "type(weights.grad), weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866dd850-b1d5-499f-830e-db0f5f7410bf",
   "metadata": {},
   "source": [
    "> Cool. Now we have the **same** grads across all weights.\n",
    "\n",
    "## Let's now make the weights *different* but the same data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bac55814-1b98-46e0-b29a-8a421f7e6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same data\n",
    "X = torch.ones(1000, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e8de034d-e3cb-4232-bb96-771fe1d511be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: Random weights\n",
    "weights = torch.randn(n_features, 1, requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a6702982-7006-4758-8646-94282fa194eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0658],\n",
       "         [-0.4783],\n",
       "         [-0.6786],\n",
       "         [ 1.3558],\n",
       "         [ 0.6845]], requires_grad=True),\n",
       " tensor([0.2683], requires_grad=True))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d64f255d-b72a-47b1-82b1-4c81b32e58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 2: forward pass\n",
    "prediction = torch.matmul(X, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "460a00b2-9bed-4c3a-9f1c-72286fbd5f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0859],\n",
       "        [1.0859],\n",
       "        [1.0859],\n",
       "        [1.0859],\n",
       "        [1.0859]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial prediction\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a489d7-2908-4692-bb2d-2992ad16d1ef",
   "metadata": {},
   "source": [
    "> Again same predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "003f2516-4803-4a77-b741-69e9ff80e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e27ff97b-6f5b-472c-8894-e4bf491f1742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "47586315-8e3a-4b25-b946-da862122f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ce6c1c50-9f90-4999-b95d-b2ad4bdb68ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " tensor([[0.1882],\n",
       "         [0.1882],\n",
       "         [0.1882],\n",
       "         [0.1882],\n",
       "         [0.1882]]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "type(weights.grad), weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23050c8e-f00e-4a2b-8d36-b69687f89d81",
   "metadata": {},
   "source": [
    "> That means, no matter what the weight values are... they will have the same grads as the data is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e6f42-22fa-4263-ac81-9b9f8836e73b",
   "metadata": {},
   "source": [
    "# Now, let's try making all values in the column the same (all rows are duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de1890f7-7eef-4ad7-bae5-484494a5a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fixed values for the features\n",
    "fixed_values = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "\n",
    "# Create a 2D array with 10000 rows and 5 features, all set to the fixed values\n",
    "X = torch.tile(fixed_values, (10000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b79c78e1-f862-444a-8d20-ec88f8c8a811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        ...,\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c6ad1ac0-d996-4292-aa87-7498b6d40a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: *CONSTANT WEIGHTS*\n",
    "weights = torch.full((n_features, 1), 0.42, requires_grad=True) # fill with constant\n",
    "bias = torch.tensor(.123, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c5d26458-a642-4130-9609-5bf2e72f7a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200]], requires_grad=True),\n",
       " tensor(0.1230, requires_grad=True))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dc4abc8e-2b79-4ec3-b527-8268af8743a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 2: forward pass\n",
    "prediction = torch.matmul(X, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "00e1ef6c-9f0c-41dd-ac4d-69a7b936404c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.4230],\n",
       "        [6.4230],\n",
       "        [6.4230],\n",
       "        [6.4230],\n",
       "        [6.4230]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial prediction\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63ea64-03ad-40c5-ae1f-6eb2708d2700",
   "metadata": {},
   "source": [
    "> Again same predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3896de8d-1b0d-4458-8b62-e3fb096bb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9b5e6cfa-3ab5-48c0-89c2-dceed440f9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "617dcc89-e825-4807-bcc0-17350e560c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e327692f-4dc3-4c6a-8b9c-b8b3bc181b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " tensor([[ 5.5253],\n",
       "         [11.0507],\n",
       "         [16.5759],\n",
       "         [22.1013],\n",
       "         [27.6265]]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "type(weights.grad), weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938814c-3c56-4174-bafd-c045b2bffd14",
   "metadata": {},
   "source": [
    "> Because each row will pass same data, but each feature will have different data. So that will make impact here. So, nothing surprising.\n",
    "\n",
    "\n",
    "# Now maing a row having the same value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8870de98-13ea-4163-9109-f81787f7ef5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     1,     1,     1,     1],\n",
       "       [    2,     2,     2,     2,     2],\n",
       "       [    3,     3,     3,     3,     3],\n",
       "       ...,\n",
       "       [ 9998,  9998,  9998,  9998,  9998],\n",
       "       [ 9999,  9999,  9999,  9999,  9999],\n",
       "       [10000, 10000, 10000, 10000, 10000]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows = 10000\n",
    "\n",
    "# Create the data where each row has the same value for all features\n",
    "X = np.array([[i] * 5 for i in range(1, n_rows + 1)])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c7462c07-6488-440e-b79c-0c8446b01815",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a1eeba79-6ee6-4aa2-9f3d-26ea311a7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: *CONSTANT WEIGHTS*\n",
    "weights = torch.full((n_features, 1), 0.42, requires_grad=True) # fill with constant\n",
    "bias = torch.tensor(.123, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "940e7e98-27a7-4b07-a481-e5529fe4d64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200]], requires_grad=True),\n",
       " tensor(0.1230, requires_grad=True))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6ad1513d-47bd-4005-9061-6596e6bd6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 2: forward pass\n",
    "prediction = torch.matmul(X, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b23543a1-1f31-4936-836e-0b2a15ddfa99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2230],\n",
       "        [ 4.3230],\n",
       "        [ 6.4230],\n",
       "        [ 8.5230],\n",
       "        [10.6230]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial prediction\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb95db8-db9c-4191-a4ef-79cdc8376b3c",
   "metadata": {},
   "source": [
    "> Again same predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4a7a497e-3655-4f5d-ab52-e0b38c720ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b32bc860-7bce-4518-8bc2-70a48913722e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "97dd52d0-351c-4588-a515-856bab6e3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6c56e68c-4fa2-4fa1-84a6-4f8d61a01583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " tensor([[70006624.],\n",
       "         [70006624.],\n",
       "         [70006624.],\n",
       "         [70006624.],\n",
       "         [70006624.]]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "type(weights.grad), weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4be921-9308-4e73-acbf-0af98c7f19fe",
   "metadata": {},
   "source": [
    "> As expected!? Exactly! Because now we are passing **the same** information in each column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e073b7-d593-4cb6-b4b4-41328b84098f",
   "metadata": {},
   "source": [
    "# Let's visualize it...\n",
    "\n",
    "Assume this is the net:\n",
    "\n",
    "<img src=\"../images/nn-constant.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad42a8-1917-41d7-ba27-57075af2dc21",
   "metadata": {},
   "source": [
    "So... let's have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cae6bc9-5925-4ff4-8420-3b8c9e2719f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.,  5.,  1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 3\n",
    "X = torch.tensor([[10, 5, 1]], dtype=torch.float32)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b5ed65f-7069-4c21-b158-d063dc432ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: *CONSTANT WEIGHTS*\n",
    "weights_1 = torch.full((n_features, 2), 1.0, requires_grad=True) # fill with constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c09725ea-3e3c-4960-be96-350ee97a7855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "679bcb20-bbe5-41a6-8745-dca1b02c44bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16., 16.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP - 2.1: forward pass\n",
    "hidden_output = torch.matmul(X, weights_1)\n",
    "hidden_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b275709-bfbf-4954-b899-56d0228a7aba",
   "metadata": {},
   "source": [
    "> The above are the intermmidiate outputs... the values in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e4dd9b1-13ad-4168-a72b-02ca563e1fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_2 = torch.full((2, 1), 1.0, requires_grad=True)\n",
    "weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44bd9d38-2363-4e84-bad5-cf44b0049824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP - 2.2: forward pass\n",
    "prediction = torch.matmul(hidden_output, weights_2)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4da10ec-c443-4e56-b770-f594472c410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95a2641f-6368-4b58-91ed-cfdc08c29288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d740cbbf-2673-4392-a6f8-742f3862102d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NoneType, NoneType)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights_1.grad), type(weights_2.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8695a504-8570-44ea-a734-df28dba7081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6b6dd65-7d7d-4987-9e21-5b51dfd8eb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[315.8000, 315.8000],\n",
       "         [157.9000, 157.9000],\n",
       "         [ 31.5800,  31.5800]]),\n",
       " tensor([[505.2800],\n",
       "         [505.2800]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "weights_1.grad, weights_2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa467b-e3aa-4435-bcb4-6d775d3da2ed",
   "metadata": {},
   "source": [
    "> Great!\n",
    "> This is as expected. All weights will get the same grad according to their respective input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc13fb8-fb2a-41b6-b207-d182830fd53a",
   "metadata": {},
   "source": [
    "# <u>KEY SUMMARY</u>:\n",
    "\n",
    "Imagine you have a neural network with just one hidden layer and two hidden units (neurons). These two neurons take in the same inputs, and they‚Äôre both connected to the output. The network is simple, but here‚Äôs where symmetry can mess things up:\n",
    "\n",
    "## üìç Symmetry Problem\n",
    "Identical Weights: Let‚Äôs say you initialize the weights (parameters) of both hidden units to the same value, like some constant ùëê. **Because they‚Äôre the same**, each hidden neuron will compute exactly the same output since they‚Äôre given the same inputs.\n",
    "\n",
    "**Forward Propagation Outcome**: When you pass inputs through the network, both neurons in the hidden layer will produce the same activation. ***This means they‚Äôre not contributing unique information to the output layer***. It‚Äôs as if you only had one neuron instead of two.\n",
    "\n",
    "**Backpropagation Issue**: During training, when you calculate gradients in backpropagation, these gradients will also be identical for each neuron‚Äôs weights because their outputs are identical. If you update the weights with these identical gradients, they‚Äôll stay the same, keeping the neurons \"stuck\" in sync.\n",
    "\n",
    "**End Result**: The two neurons continue to behave like one‚Äîthey can‚Äôt learn different features or representations from the input. So, the network isn‚Äôt using its full potential; it's as if the hidden layer has only one effective neuron.\n",
    "\n",
    "## Why It‚Äôs Bad\n",
    "When all neurons in a layer behave the same way, **you‚Äôre wasting the network‚Äôs capacity**. Ideally, each neuron should learn to detect different patterns or features in the data, helping the network perform better.\n",
    "\n",
    "## How to Fix It\n",
    "One way to avoid this is to initialize the weights randomly rather than with the same constant. Random initialization \"breaks\" the symmetry so that neurons have different starting points, allowing them to learn different features over time. Techniques like dropout regularization (which temporarily disables some neurons during training) also help break symmetry, letting the network learn in a more diverse way.\n",
    "\n",
    "## Key Takeaway\n",
    "The issue is that identical starting weights cause neurons to behave identically, effectively reducing the network‚Äôs complexity. Randomizing weights at the start helps ensure that each neuron can learn independently and contribute to a more expressive model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
