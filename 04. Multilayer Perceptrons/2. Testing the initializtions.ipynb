{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7e623c-75b7-46c1-a10b-bffca44d2602",
   "metadata": {},
   "source": [
    "# So, the thing is that --- what if we ar given constant weights!?\n",
    "\n",
    "Let's create a simple dataset, and just do a single forward pass -- then we will see how the back prop changes the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a010eab-a92e-41c7-80d2-49bdc6e47912",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6867883-1c0a-4792-a1a4-5f0549f0cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Parameters\n",
    "n_samples = 1000  # Number of samples\n",
    "n_features = 5     # Number of features\n",
    "noise = 0.1        # Standard deviation of the gaussian noise\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y, coefficients = make_regression(n_samples=n_samples, \n",
    "                                     n_features=n_features, \n",
    "                                     noise=noise, \n",
    "                                     coef=True, \n",
    "                                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d2338c6-3c91-4c38-a12a-d6bf6041618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).type(torch.float32)\n",
    "y = torch.from_numpy(y).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4c5ed49c-ecf9-4046-87a3-f0990e5e7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: *CONSTANT WEIGHTS*\n",
    "weights = torch.full((n_features, 1), 0.42, requires_grad=True) # fill with constant\n",
    "bias = torch.tensor(.123, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a66db1a-975d-49fd-9df1-12b3b960df53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200]], requires_grad=True),\n",
       " tensor(0.1230, requires_grad=True))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b016b572-3d83-4348-976f-9748d9316979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 2: forward pass\n",
    "prediction = torch.matmul(X, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38e1c673-7915-4177-a641-34cef82de824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1458],\n",
       "        [-0.1861],\n",
       "        [-0.2003],\n",
       "        [-1.0051],\n",
       "        [-1.6343]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial prediction\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "073f4767-a8a6-47ac-9062-3f3427ac3982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "581133fd-e6a1-45d3-8682-70dcf48831e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1f47886f-5868-4e77-9e3a-ca65edd9d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c3d1a329-62ae-4120-9e2f-b6d6f98d7bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " tensor([[0.3611],\n",
       "         [0.4411],\n",
       "         [0.4365],\n",
       "         [0.3894],\n",
       "         [0.4393]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "type(weights.grad), weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a933db8-d9e6-425e-9521-f189e2e9f102",
   "metadata": {},
   "source": [
    "Apart from having the **exactly the same weights** now we have different grads for each weights. That's precisely because we have different features.\n",
    "\n",
    "## Let's make the features the same as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "30d9a401-1d01-4069-aea1-f6072271fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones(1000, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4d1665f2-8f91-442f-b626-c0fd5a14a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: *CONSTANT WEIGHTS*\n",
    "weights = torch.full((n_features, 1), 0.42, requires_grad=True) # fill with constant\n",
    "bias = torch.tensor(.123, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2588d33c-05a5-4b5f-83d4-fd6f156d811d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200]], requires_grad=True),\n",
       " tensor(0.1230, requires_grad=True))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1d62c06-7f8c-460b-a71a-96a8958012c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 2: forward pass\n",
    "prediction = torch.matmul(X, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f0cd4870-b796-4af0-a602-5d74ae0199be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2230],\n",
       "        [2.2230],\n",
       "        [2.2230],\n",
       "        [2.2230],\n",
       "        [2.2230]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial prediction\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbd9f5-3790-4516-a21a-101a4f42e332",
   "metadata": {},
   "source": [
    "> See? The predictions are the same... that's expected as all values are `1`.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f692abcc-39ef-4137-89af-0b6e1afee0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5ee0f0dc-e444-47d1-a251-80fb7460cc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f08d4022-31d1-4ae2-b8ea-3fcb3f914d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "59d90340-c5d6-4d31-aa8d-c6c7378a462c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " tensor([[1.3253],\n",
       "         [1.3253],\n",
       "         [1.3253],\n",
       "         [1.3253],\n",
       "         [1.3253]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "type(weights.grad), weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866dd850-b1d5-499f-830e-db0f5f7410bf",
   "metadata": {},
   "source": [
    "> Cool. Now we have the **same** grads across all weights.\n",
    "\n",
    "## Let's now make the weights *different* but the same data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bac55814-1b98-46e0-b29a-8a421f7e6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same data\n",
    "X = torch.ones(1000, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e8de034d-e3cb-4232-bb96-771fe1d511be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: Random weights\n",
    "weights = torch.randn(n_features, 1, requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a6702982-7006-4758-8646-94282fa194eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0658],\n",
       "         [-0.4783],\n",
       "         [-0.6786],\n",
       "         [ 1.3558],\n",
       "         [ 0.6845]], requires_grad=True),\n",
       " tensor([0.2683], requires_grad=True))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d64f255d-b72a-47b1-82b1-4c81b32e58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 2: forward pass\n",
    "prediction = torch.matmul(X, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "460a00b2-9bed-4c3a-9f1c-72286fbd5f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0859],\n",
       "        [1.0859],\n",
       "        [1.0859],\n",
       "        [1.0859],\n",
       "        [1.0859]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial prediction\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a489d7-2908-4692-bb2d-2992ad16d1ef",
   "metadata": {},
   "source": [
    "> Again same predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "003f2516-4803-4a77-b741-69e9ff80e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e27ff97b-6f5b-472c-8894-e4bf491f1742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "47586315-8e3a-4b25-b946-da862122f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ce6c1c50-9f90-4999-b95d-b2ad4bdb68ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " tensor([[0.1882],\n",
       "         [0.1882],\n",
       "         [0.1882],\n",
       "         [0.1882],\n",
       "         [0.1882]]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "type(weights.grad), weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23050c8e-f00e-4a2b-8d36-b69687f89d81",
   "metadata": {},
   "source": [
    "> That means, no matter what the weight values are... they will have the same grads as the data is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e6f42-22fa-4263-ac81-9b9f8836e73b",
   "metadata": {},
   "source": [
    "# Now, let's try making all values in the column the same (all rows are duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de1890f7-7eef-4ad7-bae5-484494a5a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fixed values for the features\n",
    "fixed_values = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "\n",
    "# Create a 2D array with 10000 rows and 5 features, all set to the fixed values\n",
    "X = torch.tile(fixed_values, (10000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b79c78e1-f862-444a-8d20-ec88f8c8a811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        ...,\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c6ad1ac0-d996-4292-aa87-7498b6d40a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: *CONSTANT WEIGHTS*\n",
    "weights = torch.full((n_features, 1), 0.42, requires_grad=True) # fill with constant\n",
    "bias = torch.tensor(.123, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c5d26458-a642-4130-9609-5bf2e72f7a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200]], requires_grad=True),\n",
       " tensor(0.1230, requires_grad=True))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dc4abc8e-2b79-4ec3-b527-8268af8743a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 2: forward pass\n",
    "prediction = torch.matmul(X, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "00e1ef6c-9f0c-41dd-ac4d-69a7b936404c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.4230],\n",
       "        [6.4230],\n",
       "        [6.4230],\n",
       "        [6.4230],\n",
       "        [6.4230]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial prediction\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63ea64-03ad-40c5-ae1f-6eb2708d2700",
   "metadata": {},
   "source": [
    "> Again same predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3896de8d-1b0d-4458-8b62-e3fb096bb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9b5e6cfa-3ab5-48c0-89c2-dceed440f9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "617dcc89-e825-4807-bcc0-17350e560c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e327692f-4dc3-4c6a-8b9c-b8b3bc181b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " tensor([[ 5.5253],\n",
       "         [11.0507],\n",
       "         [16.5759],\n",
       "         [22.1013],\n",
       "         [27.6265]]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "type(weights.grad), weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938814c-3c56-4174-bafd-c045b2bffd14",
   "metadata": {},
   "source": [
    "> Because each row will pass same data, but each feature will have different data. So that will make impact here. So, nothing surprising.\n",
    "\n",
    "\n",
    "# Now maing a row having the same value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8870de98-13ea-4163-9109-f81787f7ef5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     1,     1,     1,     1],\n",
       "       [    2,     2,     2,     2,     2],\n",
       "       [    3,     3,     3,     3,     3],\n",
       "       ...,\n",
       "       [ 9998,  9998,  9998,  9998,  9998],\n",
       "       [ 9999,  9999,  9999,  9999,  9999],\n",
       "       [10000, 10000, 10000, 10000, 10000]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows = 10000\n",
    "\n",
    "# Create the data where each row has the same value for all features\n",
    "X = np.array([[i] * 5 for i in range(1, n_rows + 1)])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c7462c07-6488-440e-b79c-0c8446b01815",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a1eeba79-6ee6-4aa2-9f3d-26ea311a7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: *CONSTANT WEIGHTS*\n",
    "weights = torch.full((n_features, 1), 0.42, requires_grad=True) # fill with constant\n",
    "bias = torch.tensor(.123, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "940e7e98-27a7-4b07-a481-e5529fe4d64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200],\n",
       "         [0.4200]], requires_grad=True),\n",
       " tensor(0.1230, requires_grad=True))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6ad1513d-47bd-4005-9061-6596e6bd6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 2: forward pass\n",
    "prediction = torch.matmul(X, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b23543a1-1f31-4936-836e-0b2a15ddfa99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2230],\n",
       "        [ 4.3230],\n",
       "        [ 6.4230],\n",
       "        [ 8.5230],\n",
       "        [10.6230]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial prediction\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb95db8-db9c-4191-a4ef-79cdc8376b3c",
   "metadata": {},
   "source": [
    "> Again same predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4a7a497e-3655-4f5d-ab52-e0b38c720ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b32bc860-7bce-4518-8bc2-70a48913722e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "97dd52d0-351c-4588-a515-856bab6e3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6c56e68c-4fa2-4fa1-84a6-4f8d61a01583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " tensor([[70006624.],\n",
       "         [70006624.],\n",
       "         [70006624.],\n",
       "         [70006624.],\n",
       "         [70006624.]]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "type(weights.grad), weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4be921-9308-4e73-acbf-0af98c7f19fe",
   "metadata": {},
   "source": [
    "> As expected!? Exactly! Because now we are passing **the same** information in each column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e073b7-d593-4cb6-b4b4-41328b84098f",
   "metadata": {},
   "source": [
    "# Let's visualize it...\n",
    "\n",
    "Assume this is the net:\n",
    "\n",
    "<img src=\"../images/nn-constant.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad42a8-1917-41d7-ba27-57075af2dc21",
   "metadata": {},
   "source": [
    "So... let's have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cae6bc9-5925-4ff4-8420-3b8c9e2719f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.,  5.,  1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 3\n",
    "X = torch.tensor([[10, 5, 1]], dtype=torch.float32)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b5ed65f-7069-4c21-b158-d063dc432ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 1: *CONSTANT WEIGHTS*\n",
    "weights_1 = torch.full((n_features, 2), 1.0, requires_grad=True) # fill with constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c09725ea-3e3c-4960-be96-350ee97a7855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "679bcb20-bbe5-41a6-8745-dca1b02c44bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16., 16.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP - 2.1: forward pass\n",
    "hidden_output = torch.matmul(X, weights_1)\n",
    "hidden_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b275709-bfbf-4954-b899-56d0228a7aba",
   "metadata": {},
   "source": [
    "> The above are the intermmidiate outputs... the values in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e4dd9b1-13ad-4168-a72b-02ca563e1fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_2 = torch.full((2, 1), 1.0, requires_grad=True)\n",
    "weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44bd9d38-2363-4e84-bad5-cf44b0049824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP - 2.2: forward pass\n",
    "prediction = torch.matmul(hidden_output, weights_2)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4da10ec-c443-4e56-b770-f594472c410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95a2641f-6368-4b58-91ed-cfdc08c29288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Calculate the loss\n",
    "loss = (y - prediction) ** 2 / 2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d740cbbf-2673-4392-a6f8-742f3862102d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NoneType, NoneType)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently we don't have any grad\n",
    "type(weights_1.grad), type(weights_2.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8695a504-8570-44ea-a734-df28dba7081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 4: Calculate the grad\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6b6dd65-7d7d-4987-9e21-5b51dfd8eb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[315.8000, 315.8000],\n",
       "         [157.9000, 157.9000],\n",
       "         [ 31.5800,  31.5800]]),\n",
       " tensor([[505.2800],\n",
       "         [505.2800]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have the grad!\n",
    "weights_1.grad, weights_2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa467b-e3aa-4435-bcb4-6d775d3da2ed",
   "metadata": {},
   "source": [
    "> Great!\n",
    "> This is as expected. All weights will get the same grad according to their respective input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc13fb8-fb2a-41b6-b207-d182830fd53a",
   "metadata": {},
   "source": [
    "# <u>KEY SUMMARY</u>:\n",
    "\n",
    "Imagine you have a neural network with just one hidden layer and two hidden units (neurons). These two neurons take in the same inputs, and they’re both connected to the output. The network is simple, but here’s where symmetry can mess things up:\n",
    "\n",
    "## 📍 Symmetry Problem\n",
    "Identical Weights: Let’s say you initialize the weights (parameters) of both hidden units to the same value, like some constant 𝑐. **Because they’re the same**, each hidden neuron will compute exactly the same output since they’re given the same inputs.\n",
    "\n",
    "**Forward Propagation Outcome**: When you pass inputs through the network, both neurons in the hidden layer will produce the same activation. ***This means they’re not contributing unique information to the output layer***. It’s as if you only had one neuron instead of two.\n",
    "\n",
    "**Backpropagation Issue**: During training, when you calculate gradients in backpropagation, these gradients will also be identical for each neuron’s weights because their outputs are identical. If you update the weights with these identical gradients, they’ll stay the same, keeping the neurons \"stuck\" in sync.\n",
    "\n",
    "**End Result**: The two neurons continue to behave like one—they can’t learn different features or representations from the input. So, the network isn’t using its full potential; it's as if the hidden layer has only one effective neuron.\n",
    "\n",
    "## Why It’s Bad\n",
    "When all neurons in a layer behave the same way, **you’re wasting the network’s capacity**. Ideally, each neuron should learn to detect different patterns or features in the data, helping the network perform better.\n",
    "\n",
    "## How to Fix It\n",
    "One way to avoid this is to initialize the weights randomly rather than with the same constant. Random initialization \"breaks\" the symmetry so that neurons have different starting points, allowing them to learn different features over time. Techniques like dropout regularization (which temporarily disables some neurons during training) also help break symmetry, letting the network learn in a more diverse way.\n",
    "\n",
    "## Key Takeaway\n",
    "The issue is that identical starting weights cause neurons to behave identically, effectively reducing the network’s complexity. Randomizing weights at the start helps ensure that each neuron can learn independently and contribute to a more expressive model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
